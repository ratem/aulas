{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Time Series Characterization\n",
    "\n",
    "-   Basic Metrics (Mean, Variance, Standard Deviation, Max, Min)\n",
    "-   Kurtosis and Skewness\n",
    "-   Outliers Detection Revisited\n",
    "-   AIC and BIC Tests (continuous and discrete series)\n",
    "-   Random Walk\n",
    "\n",
    "## Basic Metrics\n",
    "\n",
    "Time series analysis often begins with calculating basic statistical\n",
    "measures to understand the central tendency, dispersion, and range of\n",
    "the data. Let’s explore these metrics:\n",
    "\n",
    "1.  **Mean**: The average value of the series.  \n",
    "    Formula: $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i$\n",
    "\n",
    "2.  **Variance**: A measure of variability that quantifies the average\n",
    "    squared deviation from the mean.  \n",
    "    Formula: $\\sigma^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2$\n",
    "\n",
    "3.  **Standard Deviation**: The square root of the variance, providing a\n",
    "    measure of dispersion in the same units as the original data.  \n",
    "    Formula: $\\sigma =\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n",
    "\n",
    "4.  **Maximum**: The highest value in the series.\n",
    "\n",
    "5.  **Minimum**: The lowest value in the series.\n",
    "\n",
    "Let’s calculate these metrics for our simulated sensor data:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Re-create the simulated sensor data\n",
    "def simulate_sensor_data(num_points=1000, min_val=0, max_val=100, missing_pct=0.01, noisy_pct=0.01, faulty_pct=0.02):\n",
    "    data = np.random.uniform(min_val, max_val, num_points)\n",
    "    missing_indices = np.random.choice(num_points, int(num_points * missing_pct), replace=False)\n",
    "    data[missing_indices] = np.nan\n",
    "    noisy_indices = np.random.choice(num_points, int(num_points * noisy_pct), replace=False)\n",
    "    data[noisy_indices] = np.random.uniform(min_val - 50, max_val + 50, len(noisy_indices))\n",
    "    faulty_start = int(num_points * (1 - faulty_pct) / 2)\n",
    "    faulty_end = int(num_points * (1 + faulty_pct) / 2)\n",
    "    data[faulty_start:faulty_end] = np.random.uniform(min_val, (max_val-min_val)/10 , faulty_end-faulty_start)\n",
    "    return data\n",
    "\n",
    "# Generate the simulated data\n",
    "simulated_data = simulate_sensor_data()\n",
    "\n",
    "# Calculate basic metrics\n",
    "mean_value = np.mean(simulated_data)\n",
    "variance = np.var(simulated_data)\n",
    "std_dev = np.std(simulated_data)\n",
    "max_value = np.max(simulated_data)\n",
    "min_value = np.min(simulated_data)\n",
    "\n",
    "print(f\"Mean: {mean_value:.2f}\")\n",
    "print(f\"Variance: {variance:.2f}\")\n",
    "print(f\"Standard Deviation: {std_dev:.2f}\")\n",
    "print(f\"Maximum: {max_value:.2f}\")\n",
    "print(f\"Minimum: {min_value:.2f}\")\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(simulated_data)\n",
    "plt.title(\"Simulated Sensor Data\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "These basic metrics provide a foundation for understanding the\n",
    "characteristics of time series data. The mean gives us the central\n",
    "tendency, while the variance and standard deviation indicate the spread\n",
    "of the data. The maximum and minimum values show the range of the\n",
    "series. In the next sections, we’ll explore more advanced statistical\n",
    "measures and techniques for characterizing time series data.\n",
    "\n",
    "## Kurtosis and Skewness\n",
    "\n",
    "Kurtosis and skewness are important statistical measures that provide\n",
    "insights into the shape and characteristics of a probability\n",
    "distribution.\n",
    "\n",
    "## Skewness\n",
    "\n",
    "Skewness measures the asymmetry of a distribution. It indicates whether\n",
    "the data is skewed to the left, right, or symmetrically distributed\n",
    "around the mean.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "-   Skewness = 0: Symmetric distribution\n",
    "\n",
    "-   Skewness \\> 0: Right-skewed (positive skew)\n",
    "\n",
    "-   Skewness \\< 0: Left-skewed (negative skew)\n",
    "\n",
    "**Formula:**  \n",
    "For a sample, the skewness is calculated as:\n",
    "\n",
    "$$\\text{Skewness}=\\frac{n}{(n-1)(n-2)}\\sum_{i=1}^n\\left(\\frac{x_i-\\bar{x}}{s}\\right)^3$$\n",
    "\n",
    "Where n is the sample size, $\\bar{x}$ is the sample mean, and s is the\n",
    "sample standard deviation.\n",
    "\n",
    "## Kurtosis\n",
    "\n",
    "Kurtosis measures the “tailedness” of a distribution. It describes the\n",
    "shape of a probability distribution’s tails in relation to its overall\n",
    "shape.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "-   Kurtosis = 3: Mesokurtic (normal distribution)\n",
    "\n",
    "-   Kurtosis \\> 3: Leptokurtic (heavy-tailed)\n",
    "\n",
    "-   Kurtosis \\< 3: Platykurtic (light-tailed)\n",
    "\n",
    "**Formula:**  \n",
    "For a sample, the kurtosis is calculated as:\n",
    "\n",
    "$$\\text{Kurtosis}=\\frac{n(n+1)}{(n-1)(n-2)(n-3)}\\sum_{i=1}^n\\left(\\frac{x_i-\\bar{x}}{s}\\right)^4-\\frac{3(n-1)^2}{(n-2)(n-3)}$$\n",
    "\n",
    "Where n is the sample size, $\\bar{x}$ is the sample mean, and s is the\n",
    "sample standard deviation.\n",
    "\n",
    "Now, let’s implement these measures in Python and examine different\n",
    "cases:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def plot_distribution(data, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=30, density=True, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Case 1: Normal distribution (symmetric, mesokurtic)\n",
    "normal_data = np.random.normal(0, 1, 10000)\n",
    "plot_distribution(normal_data, \"Normal Distribution\")\n",
    "print(\"Normal Distribution:\")\n",
    "print(f\"Skewness: {stats.skew(normal_data):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(normal_data):.4f}\\n\")\n",
    "\n",
    "# Case 2: Right-skewed distribution\n",
    "right_skewed = np.random.lognormal(0, 1, 10000)\n",
    "plot_distribution(right_skewed, \"Right-skewed Distribution\")\n",
    "print(\"Right-skewed Distribution:\")\n",
    "print(f\"Skewness: {stats.skew(right_skewed):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(right_skewed):.4f}\\n\")\n",
    "\n",
    "# Case 3: Left-skewed distribution\n",
    "left_skewed = -np.random.lognormal(0, 1, 10000)\n",
    "plot_distribution(left_skewed, \"Left-skewed Distribution\")\n",
    "print(\"Left-skewed Distribution:\")\n",
    "print(f\"Skewness: {stats.skew(left_skewed):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(left_skewed):.4f}\\n\")\n",
    "\n",
    "# Case 4: Heavy-tailed distribution (leptokurtic)\n",
    "heavy_tailed = np.random.standard_t(3, 10000)\n",
    "plot_distribution(heavy_tailed, \"Heavy-tailed Distribution\")\n",
    "print(\"Heavy-tailed Distribution:\")\n",
    "print(f\"Skewness: {stats.skew(heavy_tailed):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(heavy_tailed):.4f}\\n\")\n",
    "\n",
    "# Case 5: Light-tailed distribution (platykurtic)\n",
    "light_tailed = np.random.uniform(-3, 3, 10000)\n",
    "plot_distribution(light_tailed, \"Light-tailed Distribution\")\n",
    "print(\"Light-tailed Distribution:\")\n",
    "print(f\"Skewness: {stats.skew(light_tailed):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(light_tailed):.4f}\")\n",
    "```\n",
    "\n",
    "This code generates and analyzes five different distributions:\n",
    "\n",
    "1.  Normal distribution: Should have skewness close to 0 and kurtosis\n",
    "    close to 0 (3 if not using excess kurtosis).\n",
    "\n",
    "2.  Right-skewed distribution: Positive skewness, kurtosis \\> 0.\n",
    "\n",
    "3.  Left-skewed distribution: Negative skewness, kurtosis \\> 0.\n",
    "\n",
    "4.  Heavy-tailed distribution: Kurtosis \\> 0, potentially high.\n",
    "\n",
    "5.  Light-tailed distribution: Kurtosis \\< 0.\n",
    "\n",
    "Interpreting the results of the examples:\n",
    "\n",
    "-   Skewness values further from 0 indicate more asymmetry.\n",
    "\n",
    "-   Positive skewness suggests a longer right tail, negative skewness a\n",
    "    longer left tail.\n",
    "\n",
    "-   Kurtosis values greater than 0 (or 3 if not using excess kurtosis)\n",
    "    indicate heavier tails than a normal distribution.\n",
    "\n",
    "-   Kurtosis values less than 0 (or 3) indicate lighter tails than a\n",
    "    normal distribution.\n",
    "\n",
    "We can see that interpreting kurtosis and skewness in practical\n",
    "scenarios is crucial for understanding the characteristics of your data\n",
    "distribution.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "## Interpreting Skewness\n",
    "\n",
    "Skewness indicates the asymmetry of the distribution. In practical\n",
    "scenarios:\n",
    "\n",
    "1.  **Positive Skewness (Right-skewed)**\n",
    "\n",
    "    -   The tail of the distribution extends more towards the right.\n",
    "\n",
    "    -   Mean \\> Median \\> Mode\n",
    "\n",
    "    -   Examples:\n",
    "\n",
    "        -   Income distributions: A few high earners pull the mean\n",
    "            higher than the median.\n",
    "\n",
    "        -   House prices: A small number of very expensive houses can\n",
    "            skew the distribution.\n",
    "\n",
    "2.  **Negative Skewness (Left-skewed)**\n",
    "\n",
    "    -   The tail of the distribution extends more towards the left.\n",
    "\n",
    "    -   Mean \\< Median \\< Mode\n",
    "\n",
    "    -   Examples:\n",
    "\n",
    "        -   Age at death in developed countries: Most people live to old\n",
    "            age, with fewer early deaths.\n",
    "\n",
    "        -   Test scores with a ceiling effect: When many students score\n",
    "            near the maximum possible score.\n",
    "\n",
    "3.  **Approximately Symmetric (Skewness close to 0)**\n",
    "\n",
    "    -   The distribution is roughly symmetrical around the mean.\n",
    "\n",
    "    -   Mean ≈ Median ≈ Mode\n",
    "\n",
    "    -   Examples:\n",
    "\n",
    "        -   Height distributions in a population.\n",
    "\n",
    "        -   IQ scores (designed to be normally distributed).\n",
    "\n",
    "## Interpreting Kurtosis\n",
    "\n",
    "Kurtosis measures the tailedness of the distribution. In practical\n",
    "scenarios:\n",
    "\n",
    "1.  **High Kurtosis (Leptokurtic)**\n",
    "\n",
    "    -   Heavy tails, more outliers.\n",
    "\n",
    "    -   A sharper peak around the mean.\n",
    "\n",
    "    -   Examples:\n",
    "\n",
    "        -   Financial returns: Stock markets often exhibit extreme\n",
    "            events more frequently than a normal distribution would\n",
    "            predict.\n",
    "\n",
    "        -   Reaction time data: Often has a sharp peak with some very\n",
    "            slow reactions.\n",
    "\n",
    "2.  **Low Kurtosis (Platykurtic)**\n",
    "\n",
    "    -   Light tails, fewer outliers.\n",
    "\n",
    "    -   A flatter peak around the mean.\n",
    "\n",
    "    -   Examples:\n",
    "\n",
    "        -   Uniform distributions: Like rolling a fair die.\n",
    "\n",
    "        -   Bimodal distributions: Such as the distribution of heights\n",
    "            in a mixed-gender population.\n",
    "\n",
    "3.  **Mesokurtic (Kurtosis ≈ 3 or excess kurtosis ≈ 0)**\n",
    "\n",
    "    -   Similar to a normal distribution.\n",
    "\n",
    "    -   Examples:\n",
    "\n",
    "        -   Many natural phenomena approximately follow this\n",
    "            distribution.\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "1.  **Financial Risk Management**\n",
    "\n",
    "    -   High positive skewness in returns might indicate potential for\n",
    "        large gains.\n",
    "\n",
    "    -   High kurtosis suggests a higher risk of extreme events (both\n",
    "        positive and negative).\n",
    "\n",
    "2.  **Quality Control**\n",
    "\n",
    "    -   Negative skewness in product measurements might indicate a\n",
    "        process consistently producing near the upper specification\n",
    "        limit.\n",
    "\n",
    "    -   High kurtosis could suggest occasional defective products\n",
    "        (outliers).\n",
    "\n",
    "3.  **Environmental Science**\n",
    "\n",
    "    -   Positive skewness in pollution levels might indicate occasional\n",
    "        high pollution events.\n",
    "\n",
    "    -   Low kurtosis in rainfall data might suggest consistent,\n",
    "        predictable precipitation patterns.\n",
    "\n",
    "4.  **Healthcare**\n",
    "\n",
    "    -   Positive skewness in patient wait times could indicate a few\n",
    "        cases requiring exceptionally long waits.\n",
    "\n",
    "    -   High kurtosis in drug response data might suggest that the drug\n",
    "        has varied effects across the population.\n",
    "\n",
    "5.  **Education**\n",
    "\n",
    "    -   Negative skewness in test scores might indicate that the test\n",
    "        was too easy for the group.\n",
    "\n",
    "    -   Low kurtosis in student performance might suggest a wide range\n",
    "        of abilities in the class.\n",
    "\n",
    "To illustrate these concepts with our simulated sensor data:\n",
    "\n",
    "``` python\n",
    "# Calculate skewness and kurtosis for our simulated sensor data\n",
    "skewness = stats.skew(simulated_data)\n",
    "kurtosis = stats.kurtosis(simulated_data)\n",
    "\n",
    "print(f\"Skewness of simulated sensor data: {skewness:.4f}\")\n",
    "print(f\"Kurtosis of simulated sensor data: {kurtosis:.4f}\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(simulated_data, bins=30, density=True, alpha=0.7)\n",
    "plt.title(\"Distribution of Simulated Sensor Data\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Interpreting these results for our simulated sensor data would involve\n",
    "considering the nature of the sensor and what the measurements\n",
    "represent. For example:\n",
    "\n",
    "-   If the skewness is significantly positive, it might indicate\n",
    "    occasional high readings, perhaps due to sensor malfunctions or rare\n",
    "    environmental events.\n",
    "\n",
    "-   If the kurtosis is high, it could suggest that the sensor\n",
    "    occasionally produces extreme values, which might warrant\n",
    "    investigation into the cause of these outliers.\n",
    "\n",
    "Understanding these measures helps in identifying unusual patterns,\n",
    "potential data quality issues, or important characteristics of the\n",
    "system being measured, which can guide further analysis and\n",
    "decision-making processes.\n",
    "\n",
    "### Kurtosis and Skewness X Potential issues in data quality\n",
    "\n",
    "Kurtosis and skewness can indeed be valuable indicators of potential\n",
    "data quality issues. These statistical measures can reveal anomalies or\n",
    "unexpected patterns in your data that might warrant further\n",
    "investigation.\n",
    "\n",
    "## Skewness as an Indicator of Data Quality Issues\n",
    "\n",
    "1.  **Unexpected Asymmetry**\n",
    "\n",
    "    -   If you expect your data to be normally distributed but observe\n",
    "        significant skewness, it could indicate data quality problems.\n",
    "\n",
    "    -   For example, if sensor readings that should be symmetrical show\n",
    "        strong positive skewness, it might suggest calibration issues or\n",
    "        systematic errors in data collection.\n",
    "\n",
    "2.  **Outliers and Data Entry Errors**\n",
    "\n",
    "    -   Extreme skewness can be a sign of outliers or data entry errors.\n",
    "\n",
    "    -   A long tail in either direction might contain erroneous values\n",
    "        that need to be checked and potentially corrected or removed.\n",
    "\n",
    "3.  **Truncation or Censoring**\n",
    "\n",
    "    -   If data is truncated (cut off at a certain point) or censored\n",
    "        (values beyond a threshold are not recorded), it can lead to\n",
    "        skewness.\n",
    "\n",
    "    -   This might indicate limitations in the measurement process or\n",
    "        data collection method.\n",
    "\n",
    "## Kurtosis as an Indicator of Data Quality Issues\n",
    "\n",
    "1.  **Unexpected Peaks or Flatness**\n",
    "\n",
    "    -   Very high kurtosis (leptokurtic) might indicate the presence of\n",
    "        outliers or data spikes that could be erroneous.\n",
    "\n",
    "    -   Very low kurtosis (platykurtic) in a dataset expected to have a\n",
    "        normal distribution might suggest data compression or rounding\n",
    "        issues.\n",
    "\n",
    "2.  **Bimodality or Multimodality**\n",
    "\n",
    "    -   Kurtosis significantly lower than expected might indicate a\n",
    "        bimodal or multimodal distribution, which could suggest mixed\n",
    "        populations or systematic errors in data collection.\n",
    "\n",
    "3.  **Data Contamination**\n",
    "\n",
    "    -   Unusually high kurtosis can be a sign of data contamination,\n",
    "        where a small number of extreme values have been introduced into\n",
    "        the dataset.\n",
    "\n",
    "Let’s demonstrate how we might use these measures to identify potential\n",
    "data quality issues in our simulated sensor data:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Function to simulate sensor data with intentional quality issues\n",
    "def simulate_sensor_data_with_issues(num_points=1000, min_val=0, max_val=100, error_rate=0.05):\n",
    "    data = np.random.normal(50, 15, num_points)\n",
    "    \n",
    "    # Introduce some data quality issues\n",
    "    error_indices = np.random.choice(num_points, int(num_points * error_rate), replace=False)\n",
    "    data[error_indices] = np.random.choice([np.nan, -999, 999], size=len(error_indices))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data with issues\n",
    "data_with_issues = simulate_sensor_data_with_issues()\n",
    "\n",
    "# Calculate skewness and kurtosis\n",
    "skewness = stats.skew(data_with_issues, nan_policy='omit')\n",
    "kurtosis = stats.kurtosis(data_with_issues, nan_policy='omit')\n",
    "\n",
    "print(f\"Skewness: {skewness:.4f}\")\n",
    "print(f\"Kurtosis: {kurtosis:.4f}\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data_with_issues, bins=50, density=True, alpha=0.7)\n",
    "plt.title(\"Distribution of Sensor Data with Quality Issues\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Identify potential outliers\n",
    "Q1 = np.percentile(data_with_issues, 25)\n",
    "Q3 = np.percentile(data_with_issues, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data_with_issues[(data_with_issues < lower_bound) | (data_with_issues > upper_bound)]\n",
    "print(f\"Number of potential outliers: {len(outliers)}\")\n",
    "print(f\"Outlier values: {outliers}\")\n",
    "```\n",
    "\n",
    "In this example, we’ve intentionally introduced data quality issues such\n",
    "as missing values (NaN) and extreme outliers (-999 and 999). Here’s how\n",
    "to interpret the results:\n",
    "\n",
    "1.  **Skewness**: A significant non-zero value might indicate asymmetry\n",
    "    caused by the introduced errors.\n",
    "\n",
    "2.  **Kurtosis**: A high positive value suggests heavy tails, likely due\n",
    "    to the extreme outliers we introduced.\n",
    "\n",
    "3.  **Histogram**: Look for unexpected peaks or long tails that might\n",
    "    represent data quality issues.\n",
    "\n",
    "4.  **Outlier Detection**: The code identifies potential outliers using\n",
    "    the Interquartile Range (IQR) method. A large number of outliers or\n",
    "    extreme values could indicate data quality problems.\n",
    "\n",
    "When you observe unexpected skewness or kurtosis in your data:\n",
    "\n",
    "1.  Investigate the source of asymmetry or heavy tails.\n",
    "\n",
    "2.  Check for data entry errors, sensor malfunctions, or processing\n",
    "    issues.\n",
    "\n",
    "3.  Consider the impact of outliers and whether they represent genuine\n",
    "    extreme events or data errors.\n",
    "\n",
    "4.  Examine your data collection and processing pipeline for potential\n",
    "    sources of bias or error.\n",
    "\n",
    "Remember, while kurtosis and skewness can indicate potential issues,\n",
    "they should be used in conjunction with other data quality checks and\n",
    "domain knowledge to make informed decisions about data integrity and\n",
    "necessary cleaning steps.\n",
    "\n",
    "## Outliers Detection Revisited\n",
    "\n",
    "Outlier detection is crucial in data analysis as outliers can\n",
    "significantly impact statistical analyses and machine learning models.\n",
    "\n",
    "### Basic Concept: Quartiles\n",
    "\n",
    "Quartiles are important statistical measures used to divide a dataset\n",
    "into four equal parts. They provide valuable information about the\n",
    "distribution of data and are particularly useful for identifying\n",
    "outliers and understanding the spread of values. Let’s explore the\n",
    "concept of quartiles in more detail:\n",
    "\n",
    "## Definition and Calculation\n",
    "\n",
    "Quartiles divide a dataset into four equal parts, each containing 25% of\n",
    "the data:\n",
    "\n",
    "1.  First Quartile (Q1): The median of the lower half of the dataset\n",
    "\n",
    "2.  Second Quartile (Q2): The median of the entire dataset\n",
    "\n",
    "3.  Third Quartile (Q3): The median of the upper half of the dataset\n",
    "\n",
    "The calculation of quartiles can be done using the following steps:\n",
    "\n",
    "1.  Sort the dataset in ascending order\n",
    "\n",
    "2.  Find the median (Q2) of the entire dataset\n",
    "\n",
    "3.  Calculate Q1 as the median of the lower half of the data\n",
    "\n",
    "4.  Calculate Q3 as the median of the upper half of the data\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Quartiles are often visualized using box plots (also known as\n",
    "box-and-whisker plots). These plots provide a graphical representation\n",
    "of the dataset’s distribution, showing:\n",
    "\n",
    "-   The minimum and maximum values (whiskers)\n",
    "\n",
    "-   The first and third quartiles (edges of the box)\n",
    "\n",
    "-   The median (line inside the box)\n",
    "\n",
    "-   Potential outliers (individual points beyond the whiskers)\n",
    "\n",
    "## Applications\n",
    "\n",
    "Quartiles have various applications in data analysis and statistics:\n",
    "\n",
    "1.  Summarizing data distribution\n",
    "\n",
    "2.  Comparing multiple datasets\n",
    "\n",
    "3.  Identifying skewness in data\n",
    "\n",
    "4.  Detecting outliers\n",
    "\n",
    "5.  Providing input for further statistical analyses\n",
    "\n",
    "#### Quartiles X Percentiles\n",
    "\n",
    "Quartiles and percentiles are both measures used to divide a dataset\n",
    "into segments, but they differ in their granularity and specific\n",
    "applications. Let’s explore the key differences between these two\n",
    "concepts:\n",
    "\n",
    "## Definition\n",
    "\n",
    "**Quartiles:**\n",
    "\n",
    "-   Divide the dataset into four equal parts (quarters).\n",
    "\n",
    "-   There are three quartile points: Q1 (25th percentile), Q2 (50th\n",
    "    percentile or median), and Q3 (75th percentile).\n",
    "\n",
    "**Percentiles:**\n",
    "\n",
    "-   Divide the dataset into 100 equal parts.\n",
    "\n",
    "-   There are 99 percentile points, ranging from the 1st to the 99th\n",
    "    percentile.\n",
    "\n",
    "## Granularity\n",
    "\n",
    "**Quartiles:**\n",
    "\n",
    "-   Provide a coarser division of the data (4 parts).\n",
    "\n",
    "-   Useful for quick, broad insights into data distribution.\n",
    "\n",
    "**Percentiles:**\n",
    "\n",
    "-   Offer a much finer division of the data (100 parts).\n",
    "\n",
    "-   Allow for more precise positioning of data points within the\n",
    "    distribution.\n",
    "\n",
    "## Calculation\n",
    "\n",
    "**Quartiles:**\n",
    "\n",
    "-   Q1 is the 25th percentile\n",
    "\n",
    "-   Q2 (median) is the 50th percentile\n",
    "\n",
    "-   Q3 is the 75th percentile\n",
    "\n",
    "**Percentiles:**\n",
    "\n",
    "-   Can be any value from 1 to 99\n",
    "\n",
    "-   The 50th percentile is equivalent to the median (Q2)\n",
    "\n",
    "## Applications\n",
    "\n",
    "**Quartiles:**\n",
    "\n",
    "-   Commonly used in box plots to visualize data distribution\n",
    "\n",
    "-   Used to calculate the Interquartile Range (IQR) for outlier\n",
    "    detection\n",
    "\n",
    "-   Provide a quick summary of data spread and central tendency\n",
    "\n",
    "**Percentiles:**\n",
    "\n",
    "-   Used in standardized testing (e.g., “scoring in the 90th\n",
    "    percentile”)\n",
    "\n",
    "-   Applied in growth charts (e.g., child height/weight percentiles)\n",
    "\n",
    "-   Useful for setting thresholds or benchmarks in various fields\n",
    "\n",
    "## Example\n",
    "\n",
    "Let’s illustrate the difference with a Python example:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(100, 15, 1000)\n",
    "\n",
    "# Calculate quartiles\n",
    "Q1, Q2, Q3 = np.percentile(data, [25, 50, 75])\n",
    "\n",
    "# Calculate some percentiles\n",
    "P10, P90, P95 = np.percentile(data, [10, 90, 95])\n",
    "\n",
    "print(f\"Quartiles: Q1={Q1:.2f}, Q2={Q2:.2f}, Q3={Q3:.2f}\")\n",
    "print(f\"Selected Percentiles: P10={P10:.2f}, P90={P90:.2f}, P95={P95:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, bins=30, edgecolor='black')\n",
    "plt.axvline(Q1, color='r', linestyle='dashed', linewidth=2, label='Q1')\n",
    "plt.axvline(Q2, color='g', linestyle='dashed', linewidth=2, label='Q2 (Median)')\n",
    "plt.axvline(Q3, color='b', linestyle='dashed', linewidth=2, label='Q3')\n",
    "plt.axvline(P10, color='m', linestyle='dotted', linewidth=2, label='10th Percentile')\n",
    "plt.axvline(P90, color='c', linestyle='dotted', linewidth=2, label='90th Percentile')\n",
    "plt.legend()\n",
    "plt.title('Distribution with Quartiles and Selected Percentiles')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This example demonstrates how quartiles provide a broader division of\n",
    "the data, while percentiles offer more granular insights. The\n",
    "visualization helps to see how these measures relate to the overall\n",
    "distribution of the data. In summary, while quartiles and percentiles\n",
    "are related concepts, they differ in their level of detail and specific\n",
    "use cases. Quartiles are more commonly used for general data\n",
    "summarization and visualization, while percentiles offer more precise\n",
    "positioning within a distribution, making them valuable for detailed\n",
    "analysis and benchmarking.\n",
    "\n",
    "### Outlier Detection Methods\n",
    "\n",
    "## 1. Interquartile Range (IQR) Method\n",
    "\n",
    "The IQR method is a robust technique for identifying outliers based on\n",
    "the distribution of the data.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "-   IQR = Q3 - Q1\n",
    "\n",
    "-   Lower Bound = Q1 - 1.5 \\* IQR\n",
    "\n",
    "-   Upper Bound = Q3 + 1.5 \\* IQR\n",
    "\n",
    "Where Q1 is the first quartile (25th percentile) and Q3 is the third\n",
    "quartile (75th percentile).\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def iqr_outliers(data):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.concatenate([np.random.normal(0, 1, 1000), np.random.uniform(-10, 10, 50)])\n",
    "\n",
    "outliers, lower_bound, upper_bound = iqr_outliers(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data)\n",
    "plt.title(\"Box Plot with Outliers\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "print(f\"Lower bound: {lower_bound:.2f}\")\n",
    "print(f\"Upper bound: {upper_bound:.2f}\")\n",
    "```\n",
    "\n",
    "## 2. Z-Score Method\n",
    "\n",
    "The Z-score method identifies outliers based on how many standard\n",
    "deviations a data point is from the mean.\n",
    "\n",
    "**Formula:**  \n",
    "Z-score = (X - μ) / σ\n",
    "\n",
    "Where X is the data point, μ is the mean, and σ is the standard\n",
    "deviation.\n",
    "\n",
    "``` python\n",
    "from scipy import stats\n",
    "\n",
    "def zscore_outliers(data, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "z_outliers = zscore_outliers(data)\n",
    "print(f\"Number of outliers detected (Z-score): {len(z_outliers)}\")\n",
    "```\n",
    "\n",
    "## 3. Modified Z-Score Method\n",
    "\n",
    "This method is more robust to extreme values as it uses the median and\n",
    "Median Absolute Deviation (MAD) instead of mean and standard deviation.\n",
    "\n",
    "**Formula:**  \n",
    "Modified Z-score = 0.6745 \\* (X - median) / MAD\n",
    "\n",
    "``` python\n",
    "def modified_zscore_outliers(data, threshold=3.5):\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    outliers = data[np.abs(modified_z_scores) > threshold]\n",
    "    return outliers\n",
    "\n",
    "mod_z_outliers = modified_zscore_outliers(data)\n",
    "print(f\"Number of outliers detected (Modified Z-score): {len(mod_z_outliers)}\")\n",
    "```\n",
    "\n",
    "## 4. Tukey’s Method (Boxplot Method)\n",
    "\n",
    "This method is similar to the IQR method but uses different multipliers\n",
    "for the whiskers.\n",
    "\n",
    "``` python\n",
    "def tukey_outliers(data, k=1.5):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "tukey_out = tukey_outliers(data)\n",
    "print(f\"Number of outliers detected (Tukey's method): {len(tukey_out)}\")\n",
    "```\n",
    "\n",
    "## 5. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "DBSCAN is a clustering algorithm that can be used for outlier detection.\n",
    "It identifies outliers as points that are in low-density regions.\n",
    "\n",
    "``` python\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def dbscan_outliers(data, eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    outlier_labels = dbscan.fit_predict(data.reshape(-1, 1))\n",
    "    outliers = data[outlier_labels == -1]\n",
    "    return outliers\n",
    "\n",
    "dbscan_out = dbscan_outliers(data)\n",
    "print(f\"Number of outliers detected (DBSCAN): {len(dbscan_out)}\")\n",
    "```\n",
    "\n",
    "## Comparison of Methods\n",
    "\n",
    "Let’s compare the results of these methods:\n",
    "\n",
    "``` python\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(range(len(data)), data, c='blue', alpha=0.5, label='Data')\n",
    "plt.scatter(range(len(data)), outliers, c='red', label='IQR Outliers')\n",
    "plt.scatter(range(len(data)), z_outliers, c='green', label='Z-Score Outliers')\n",
    "plt.scatter(range(len(data)), mod_z_outliers, c='orange', label='Modified Z-Score Outliers')\n",
    "plt.scatter(range(len(data)), tukey_out, c='purple', label='Tukey Outliers')\n",
    "plt.scatter(range(len(data)), dbscan_out, c='brown', label='DBSCAN Outliers')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Outlier Detection Methods')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Index')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Each method has its strengths and weaknesses:\n",
    "\n",
    "-   IQR and Tukey’s method are robust to extreme values but may not work\n",
    "    well for non-normal distributions.\n",
    "\n",
    "-   Z-score is simple but assumes a normal distribution and is sensitive\n",
    "    to extreme outliers.\n",
    "\n",
    "-   Modified Z-score is more robust than the standard Z-score for\n",
    "    non-normal distributions.\n",
    "\n",
    "-   DBSCAN can detect outliers in multidimensional space and doesn’t\n",
    "    assume any particular distribution, but it’s sensitive to parameter\n",
    "    choices.\n",
    "\n",
    "When dealing with time series data, consider:\n",
    "\n",
    "1.  Using rolling windows to detect local outliers.\n",
    "\n",
    "2.  Applying seasonal decomposition before outlier detection.\n",
    "\n",
    "3.  Using domain-specific thresholds based on the nature of your data.\n",
    "\n",
    "Remember, the choice of method depends on your data’s characteristics\n",
    "and the specific requirements of your analysis. It’s often beneficial to\n",
    "use multiple methods and compare their results.\n",
    "\n",
    "**Sources:** - (1) Data-Preprocessing.ipynb - [(2) Quartile: Definition,\n",
    "Finding, and Using - Statistics By\n",
    "Jim](https://statisticsbyjim.com/basics/quartile/) - [(3) Quartiles and\n",
    "Their Importance in Data Analysis - Algor\n",
    "Cards](https://cards.algoreducation.com/en/content/F-QNSGE2/understanding-quartiles-data-analysis) -\n",
    "[(4) Quartiles - Definition, Formulas, Interquartile Range -\n",
    "BYJU’S](https://byjus.com/maths/quartiles/) - [(5) What Are Quartiles?\n",
    "Statistics 101 \\|\n",
    "Outlier](https://articles.outlier.org/what-are-quartiles-in-statistics) -\n",
    "[(6) Quartiles & Quantiles \\| Calculation, Definition & Interpretation -\n",
    "Scribbr](https://www.scribbr.com/statistics/quartiles-quantiles/) - [(7)\n",
    "What Is a Quartile? How It Works and Example -\n",
    "Investopedia](https://www.investopedia.com/terms/q/quartile.asp) - [(8)\n",
    "How to Find Quartiles (Even Set of Data) \\| Math with Mr. J -\n",
    "YouTube](https://www.youtube.com/embed/AfSUzcv3K04?autoplay=1&color=white&playsinline=true&enablejsapi=1&origin=https%3A%2F%2Fwww.perplexity.ai&widgetid=3) -\n",
    "[(9) .5.1 Calculating the range and interquartile\n",
    "range](https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch12/5214890-eng.htm) -\n",
    "[(10) Quartiles & Interquartile Range \\| Calculation & Examples -\n",
    "Study.com](https://study.com/learn/lesson/how-to-find-interquartile-range-quartiles.html) -\n",
    "[(11) Quartiles - Definition, Formulas, Interquartile Range -\n",
    "BYJU’S](https://byjus.com/maths/quartiles/) - [(12) Quartiles - Math is\n",
    "Fun](https://www.mathsisfun.com/data/quartiles.html) - [(13) Quartile\n",
    "Formula - Cuemath](https://www.cuemath.com/quartile-formula/) - [(14)\n",
    "Quartiles & Quantiles \\| Calculation, Definition & Interpretation -\n",
    "Scribbr](https://www.scribbr.com/statistics/quartiles-quantiles/) -\n",
    "[(15) What are Quartiles? - Statistics How\n",
    "To](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/what-are-quartiles/) -\n",
    "[(16) What Is a Quartile? How It Works and Example -\n",
    "Investopedia](https://www.investopedia.com/terms/q/quartile.asp) - [(17)\n",
    "Quartile - GCSE Maths - Steps, Examples &\n",
    "Worksheet](https://thirdspacelearning.com/gcse-maths/statistics/quartile/)\n",
    "\n",
    "## AIC and BIC Tests\n",
    "\n",
    "Akaike Information Criterion (AIC) and Bayesian Information Criterion\n",
    "(BIC) are important model selection criteria used in statistics and\n",
    "machine learning. They help in comparing different models and selecting\n",
    "the one that best balances goodness of fit with model complexity. Both\n",
    "AIC and BIC are particularly useful when dealing with time series data,\n",
    "including both continuous and discrete series.\n",
    "\n",
    "## Akaike Information Criterion (AIC)\n",
    "\n",
    "AIC is defined as:\n",
    "\n",
    "$AIC=2k-2\\ln(\\hat{L})$\n",
    "\n",
    "Where:\n",
    "\n",
    "-   k is the number of parameters in the model\n",
    "\n",
    "-   $\\hat{L}$ is the maximum value of the likelihood function for the\n",
    "    model\n",
    "\n",
    "The AIC penalizes the addition of parameters to discourage overfitting.\n",
    "The model with the lowest AIC is preferred.\n",
    "\n",
    "## Bayesian Information Criterion (BIC)\n",
    "\n",
    "BIC is defined as:\n",
    "\n",
    "$BIC=\\ln(n)k-2\\ln(\\hat{L})$\n",
    "\n",
    "Where:\n",
    "\n",
    "-   n is the number of observations\n",
    "\n",
    "-   k is the number of parameters\n",
    "\n",
    "-   $\\hat{L}$ is the maximum value of the likelihood function for the\n",
    "    model\n",
    "\n",
    "BIC penalizes model complexity more heavily than AIC, especially for\n",
    "large sample sizes.\n",
    "\n",
    "## Importance\n",
    "\n",
    "1.  Model Selection: Both criteria help in selecting the best model from\n",
    "    a set of candidate models.\n",
    "\n",
    "2.  Overfitting Prevention: They penalize complex models to prevent\n",
    "    overfitting.\n",
    "\n",
    "3.  Balancing Fit and Complexity: They provide a trade-off between model\n",
    "    fit and complexity.\n",
    "\n",
    "4.  Comparison of Non-Nested Models: Unlike likelihood ratio tests, AIC\n",
    "    and BIC can compare non-nested models.\n",
    "\n",
    "## Differences between AIC and BIC\n",
    "\n",
    "1.  Penalty Term: BIC penalizes model complexity more heavily,\n",
    "    especially for large sample sizes.\n",
    "\n",
    "2.  Consistency: BIC is consistent (will select the true model as sample\n",
    "    size approaches infinity), while AIC is not.\n",
    "\n",
    "3.  Model Selection: AIC tends to select more complex models, while BIC\n",
    "    tends to select simpler models.\n",
    "\n",
    "## Example Code\n",
    "\n",
    "Let’s use AIC and BIC to compare different ARIMA models for our\n",
    "simulated sensor data:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Re-create the simulated sensor data\n",
    "def simulate_sensor_data(num_points=1000, min_val=0, max_val=100, missing_pct=0.01, noisy_pct=0.01, faulty_pct=0.02):\n",
    "    data = np.random.uniform(min_val, max_val, num_points)\n",
    "    missing_indices = np.random.choice(num_points, int(num_points * missing_pct), replace=False)\n",
    "    data[missing_indices] = np.nan\n",
    "    noisy_indices = np.random.choice(num_points, int(num_points * noisy_pct), replace=False)\n",
    "    data[noisy_indices] = np.random.uniform(min_val - 50, max_val + 50, len(noisy_indices))\n",
    "    faulty_start = int(num_points * (1 - faulty_pct) / 2)\n",
    "    faulty_end = int(num_points * (1 + faulty_pct) / 2)\n",
    "    data[faulty_start:faulty_end] = np.random.uniform(min_val, (max_val-min_val)/10 , faulty_end-faulty_start)\n",
    "    return data\n",
    "\n",
    "# Generate the simulated data\n",
    "simulated_data = simulate_sensor_data()\n",
    "\n",
    "# Remove NaN values\n",
    "simulated_data = simulated_data[~np.isnan(simulated_data)]\n",
    "\n",
    "# Function to calculate AIC and BIC for ARIMA models\n",
    "def evaluate_arima_model(data, order):\n",
    "    model = ARIMA(data, order=order)\n",
    "    results = model.fit()\n",
    "    return results.aic, results.bic\n",
    "\n",
    "# Test different ARIMA models\n",
    "p_values = range(0, 3)\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 3)\n",
    "\n",
    "best_aic = float(\"inf\")\n",
    "best_bic = float(\"inf\")\n",
    "best_order_aic = None\n",
    "best_order_bic = None\n",
    "\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            try:\n",
    "                aic, bic = evaluate_arima_model(simulated_data, (p,d,q))\n",
    "                if aic < best_aic:\n",
    "                    best_aic = aic\n",
    "                    best_order_aic = (p,d,q)\n",
    "                if bic < best_bic:\n",
    "                    best_bic = bic\n",
    "                    best_order_bic = (p,d,q)\n",
    "                print(f'ARIMA({p},{d},{q}) - AIC: {aic}, BIC: {bic}')\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print(f'\\nBest AIC: {best_aic}, Order: {best_order_aic}')\n",
    "print(f'Best BIC: {best_bic}, Order: {best_order_bic}')\n",
    "```\n",
    "\n",
    "This code will compare different ARIMA models using both AIC and BIC.\n",
    "The model with the lowest AIC and BIC will be selected as the best model\n",
    "according to each criterion.\n",
    "\n",
    "## Interpretation of Results\n",
    "\n",
    "1.  The model with the lowest AIC is considered the best according to\n",
    "    the AIC criterion.\n",
    "\n",
    "2.  The model with the lowest BIC is considered the best according to\n",
    "    the BIC criterion.\n",
    "\n",
    "3.  If AIC and BIC select different models, it’s often useful to\n",
    "    consider both and potentially use other criteria or domain knowledge\n",
    "    to make the final selection.\n",
    "\n",
    "4.  Generally, if the sample size is large, BIC might be preferred as it\n",
    "    tends to select simpler models.\n",
    "\n",
    "5.  If prediction is the main goal, AIC might be preferred as it\n",
    "    asymptotically selects the model with the least mean squared error.\n",
    "\n",
    "In practice, both AIC and BIC should be considered alongside other model\n",
    "diagnostics and domain knowledge when selecting the final model. They\n",
    "provide valuable information about the trade-off between model fit and\n",
    "complexity, helping to prevent overfitting and select parsimonious\n",
    "models.\n",
    "\n",
    "### Hyperparameter Tunning\n",
    "\n",
    "AIC and BIC are powerful tools for model selection and hyperparameter\n",
    "tuning. These criteria help balance model complexity with goodness of\n",
    "fit, allowing us to compare different sets of hyperparameters for a\n",
    "given model. Let’s explore how to use AIC and BIC for hyperparameter\n",
    "selection, including a practical example with visualization.\n",
    "\n",
    "## Using AIC and BIC for Hyperparameter Selection\n",
    "\n",
    "To use AIC and BIC for hyperparameter selection:\n",
    "\n",
    "1.  Train models with different sets of hyperparameters\n",
    "\n",
    "2.  Calculate AIC and BIC for each model\n",
    "\n",
    "3.  Compare the AIC and BIC values\n",
    "\n",
    "4.  Select the model with the lowest AIC or BIC\n",
    "\n",
    "The model with the lowest AIC or BIC value is considered the best\n",
    "balance between model complexity and goodness of fit.\n",
    "\n",
    "#### Example: ARIMA Model Hyperparameter Selection\n",
    "\n",
    "ARIMA stands for Autoregressive Integrated Moving Average. It’s a\n",
    "versatile model for forecasting time series data, especially useful for\n",
    "data exhibiting trends or non-stationarity, like random walks. Let’s\n",
    "break down the components:\n",
    "\n",
    "1.  AR (Autoregressive): The ‘p’ in ARIMA(p,d,q)\n",
    "\n",
    "    -   Represents the number of lag observations in the model.\n",
    "\n",
    "    -   In a random walk, typically p = 0, as future values don’t depend\n",
    "        on past values beyond the most recent one.\n",
    "\n",
    "2.  I (Integrated): The ‘d’ in ARIMA(p,d,q)\n",
    "\n",
    "    -   Represents the degree of differencing required to make the\n",
    "        series stationary.\n",
    "\n",
    "    -   For a random walk, d = 1, as one level of differencing makes it\n",
    "        stationary.\n",
    "\n",
    "3.  MA (Moving Average): The ‘q’ in ARIMA(p,d,q)\n",
    "\n",
    "    -   Represents the number of lagged forecast errors in the\n",
    "        prediction equation.\n",
    "\n",
    "    -   In a simple random walk, typically q = 0.\n",
    "\n",
    "Let’s use an example of selecting hyperparameters for an ARIMA model\n",
    "using AIC and BIC. We’ll use the simulated sensor data from previous\n",
    "examples.\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Re-create the simulated sensor data\n",
    "def simulate_sensor_data(num_points=1000, min_val=0, max_val=100, missing_pct=0.01, noisy_pct=0.01, faulty_pct=0.02):\n",
    "    data = np.random.uniform(min_val, max_val, num_points)\n",
    "    missing_indices = np.random.choice(num_points, int(num_points * missing_pct), replace=False)\n",
    "    data[missing_indices] = np.nan\n",
    "    noisy_indices = np.random.choice(num_points, int(num_points * noisy_pct), replace=False)\n",
    "    data[noisy_indices] = np.random.uniform(min_val - 50, max_val + 50, len(noisy_indices))\n",
    "    faulty_start = int(num_points * (1 - faulty_pct) / 2)\n",
    "    faulty_end = int(num_points * (1 + faulty_pct) / 2)\n",
    "    data[faulty_start:faulty_end] = np.random.uniform(min_val, (max_val-min_val)/10 , faulty_end-faulty_start)\n",
    "    return data\n",
    "\n",
    "# Generate the simulated data\n",
    "simulated_data = simulate_sensor_data()\n",
    "\n",
    "# Remove NaN values\n",
    "simulated_data = simulated_data[~np.isnan(simulated_data)]\n",
    "\n",
    "# Function to calculate AIC and BIC for ARIMA models\n",
    "def evaluate_arima_model(data, order):\n",
    "    model = ARIMA(data, order=order)\n",
    "    results = model.fit()\n",
    "    return results.aic, results.bic\n",
    "\n",
    "# Test different ARIMA models\n",
    "p_values = range(0, 3)\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 3)\n",
    "\n",
    "results = []\n",
    "\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            try:\n",
    "                aic, bic = evaluate_arima_model(simulated_data, (p,d,q))\n",
    "                results.append(((p,d,q), aic, bic))\n",
    "                print(f'ARIMA({p},{d},{q}) - AIC: {aic}, BIC: {bic}')\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results, columns=['order', 'AIC', 'BIC'])\n",
    "df_results[['p', 'd', 'q']] = pd.DataFrame(df_results['order'].tolist(), index=df_results.index)\n",
    "\n",
    "# Plot AIC and BIC\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(df_results)), df_results['AIC'], label='AIC')\n",
    "plt.plot(range(len(df_results)), df_results['BIC'], label='BIC')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Criterion Value')\n",
    "plt.title('AIC and BIC for Different ARIMA Models')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(df_results)), [f'({p},{d},{q})' for p,d,q in df_results['order']], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the best models according to AIC and BIC\n",
    "best_aic = df_results.loc[df_results['AIC'].idxmin()]\n",
    "best_bic = df_results.loc[df_results['BIC'].idxmin()]\n",
    "\n",
    "print(f\"Best model according to AIC: ARIMA{best_aic['order']} with AIC = {best_aic['AIC']}\")\n",
    "print(f\"Best model according to BIC: ARIMA{best_bic['order']} with BIC = {best_bic['BIC']}\")\n",
    "```\n",
    "\n",
    "This code will generate a plot showing AIC and BIC values for different\n",
    "ARIMA models with varying hyperparameters (p, d, q). The model with the\n",
    "lowest AIC or BIC value is considered the best.\n",
    "\n",
    "## Interpreting the Results\n",
    "\n",
    "1.  The plot shows AIC and BIC values for different ARIMA models.\n",
    "\n",
    "2.  Lower values indicate better models.\n",
    "\n",
    "3.  The model with the lowest AIC may differ from the model with the\n",
    "    lowest BIC.\n",
    "\n",
    "4.  BIC tends to penalize model complexity more heavily, especially for\n",
    "    larger sample sizes.\n",
    "\n",
    "When interpreting the results:\n",
    "\n",
    "-   Look for the lowest points on the AIC and BIC curves.\n",
    "\n",
    "-   Consider both AIC and BIC, as they may suggest different optimal\n",
    "    models.\n",
    "\n",
    "-   If AIC and BIC disagree, consider your specific needs:\n",
    "\n",
    "    -   AIC may be preferred for prediction tasks.\n",
    "\n",
    "    -   BIC may be preferred for explanation tasks or when seeking a\n",
    "        simpler model.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "1.  Simplistic complexity penalization: Both AIC and BIC penalize model\n",
    "    complexity based solely on the number of parameters, which may not\n",
    "    accurately reflect the true complexity of a model\n",
    "    [1](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/model-selection.pdf).\n",
    "    This can be problematic for models where the effective number of\n",
    "    parameters is not easily determined.\n",
    "\n",
    "2.  Asymptotic assumptions: AIC and BIC are based on asymptotic theory,\n",
    "    assuming large sample sizes. They may not perform well with small\n",
    "    datasets\n",
    "    [2](https://stats.stackexchange.com/questions/591337/aic-when-not-to-use-it).\n",
    "    AIC, in particular, has been reported to perform poorly for small\n",
    "    numbers of data points.\n",
    "\n",
    "3.  Model set limitations: These criteria assume that the true model is\n",
    "    within the set of models being compared. If the true model is not\n",
    "    included, the results may be misleading\n",
    "    [2](https://stats.stackexchange.com/questions/591337/aic-when-not-to-use-it).\n",
    "\n",
    "4.  Lack of uncertainty consideration: AIC and BIC do not take into\n",
    "    account the uncertainty of the model\n",
    "    [6](https://www.machinelearningmastery.com/probabilistic-model-selection-measures/).\n",
    "    They provide point estimates rather than full posterior\n",
    "    distributions over models.\n",
    "\n",
    "5.  Consistency issues: In the large-data limit, AIC tends to select\n",
    "    more complicated models than BIC and is not necessarily\n",
    "    asymptotically consistent if the true model is in the set\n",
    "    [2](https://stats.stackexchange.com/questions/591337/aic-when-not-to-use-it).\n",
    "\n",
    "6.  Assumptions about data generation: Both criteria assume that the\n",
    "    data are actually generated by the model being evaluated, which may\n",
    "    not always be true in real-world scenarios\n",
    "    [5](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html).\n",
    "\n",
    "7.  Breakdown in high-dimensional settings: These models tend to break\n",
    "    down when the problem is badly conditioned, such as when there are\n",
    "    more features than samples\n",
    "    [5](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html).\n",
    "\n",
    "8.  Limited applicability: In some cases, such as with non-nested models\n",
    "    or when comparing models with different likelihood functions, AIC\n",
    "    and BIC may not be appropriate\n",
    "    [1](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/model-selection.pdf).\n",
    "\n",
    "9.  Sensitivity to heterogeneity: The performance of AIC and BIC can\n",
    "    vary depending on the level of unobserved heterogeneity in the data\n",
    "    [8](https://www.researchgate.net/publication/303954547_The_relative_performance_of_AIC_AIC_C_and_BIC_in_the_presence_of_unobserved_heterogeneity).\n",
    "\n",
    "Given these limitations, it’s often recommended to use multiple model\n",
    "selection criteria, cross-validation, or more sophisticated methods like\n",
    "the variational Free Energy for hyperparameter tuning, especially in\n",
    "complex modeling scenarios\n",
    "[3](https://pmc.ncbi.nlm.nih.gov/articles/PMC3200437/).\n",
    "\n",
    "**Sources:** - (1) Data-Preprocessing.ipynb - [(2) AIC based model\n",
    "selection, hyperparameter optimization and in\n",
    "…](https://stats.stackexchange.com/questions/377321/aic-based-model-selection-hyperparameter-optimization-and-in-sample-prediction) -\n",
    "[(3) Lasso model selection: AIC-BIC / cross-validation -\n",
    "Scikit-learn](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html) -\n",
    "[(4) .2. Tuning the hyper-parameters of an estimator -\n",
    "Scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html) -\n",
    "[(5) Hyperparameters - Florent\n",
    "Brunet](https://www.brnt.eu/phd/node14.html) - [(6) Comparison of Akaike\n",
    "information criterion (AIC) and Bayesian\n",
    "…](https://www.researchgate.net/publication/222021636_Comparison_of_Akaike_information_criterion_AIC_and_Bayesian_information_criterion_BIC_in_selection_of_stock-recruitment_relationship) -\n",
    "[(7) Comparison of AIC and BIC for the selection of a best-fit model for\n",
    "…](https://www.researchgate.net/figure/Comparison-of-AIC-and-BIC-for-the-selection-of-a-best-fit-model-for-SPI3_tbl2_343320941) -\n",
    "[(8) Probabilistic Model Selection with AIC, BIC, and\n",
    "MDL](https://www.machinelearningmastery.com/probabilistic-model-selection-measures/) -\n",
    "[(9) Lasso Regression Hyperparameter Optimization \\| Machine\n",
    "Learning](https://labex.io/tutorials/ml-model-selection-for-lasso-regression-49192) -\n",
    "[(10) Complete 4 Time Series Forecasting Tuned Models -\n",
    "Kaggle](https://www.kaggle.com/code/amiribrahimtaj/complete-4-time-series-forecasting-tuned-models) -\n",
    "[(11) Model selection using AIC and BIC in python -\n",
    "Kaggle](https://www.kaggle.com/code/gourab1992/model-selection-using-aic-and-bic-in-python) -\n",
    "[(12) The relative performance of AIC, AICC and BIC in the presence of\n",
    "…](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210x.12541) -\n",
    "[(13) Hyperparameter Optimization in Machine Learning -\n",
    "arXiv](https://arxiv.org/html/2410.22854v1) - [(14) Hyperparameters -\n",
    "Florent Brunet](https://www.brnt.eu/phd/node14.html) - [(15) Lasso model\n",
    "selection: AIC-BIC / cross-validation -\n",
    "Scikit-learn](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html) -\n",
    "[(16) AIC based model selection, hyperparameter optimization and in\n",
    "…](https://stats.stackexchange.com/questions/377321/aic-based-model-selection-hyperparameter-optimization-and-in-sample-prediction) -\n",
    "[(17) Akaike Information Criterion \\| When & How to Use It\n",
    "(Example)](https://www.scribbr.com/statistics/akaike-information-criterion/) -\n",
    "[(18) .2. Tuning the hyper-parameters of an estimator -\n",
    "Scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "## Random Walks\n",
    "\n",
    "## Definition of Random Walk\n",
    "\n",
    "A Random Walk is a mathematical object that describes a path consisting\n",
    "of a succession of random steps. <br> In the context of time series, it\n",
    "refers to a time series where the change from one period to the next is\n",
    "random.\n",
    "\n",
    "## Types of Random Walks\n",
    "\n",
    "1.  **Simple Random Walk**: Each step is independent and can be either\n",
    "    up or down with equal probability.\n",
    "\n",
    "2.  **Random Walk with Drift**: Similar to a simple random walk, but\n",
    "    with an added constant term (drift) that biases the walk in a\n",
    "    particular direction.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "## Simple Random Walk\n",
    "\n",
    "For a simple random walk, we can define it as:\n",
    "\n",
    "$X_t=X_{t-1}+\\epsilon_t$\n",
    "\n",
    "Where:\n",
    "\n",
    "-   $X\\_t$ is the value at time t\n",
    "\n",
    "-   $\\\\epsilon\\_t$ is a random variable (often assumed to be normally\n",
    "    distributed with mean 0 and constant variance)\n",
    "\n",
    "## Random Walk with Drift\n",
    "\n",
    "For a random walk with drift, we add a constant term:\n",
    "\n",
    "$X_t=\\delta +X_{t-1}+\\epsilon_t$\n",
    "\n",
    "Where $\\\\delta$ is the drift parameter.\n",
    "\n",
    "## Relationship with Time Series Analysis\n",
    "\n",
    "Random Walks are crucial in time series analysis for several reasons:\n",
    "\n",
    "1.  **Non-stationarity**: Random walks are non-stationary processes,\n",
    "    meaning their statistical properties change over time. This is\n",
    "    important because many time series models assume stationarity.\n",
    "\n",
    "2.  **Unit Root Tests**: Tests for random walks (like the Augmented\n",
    "    Dickey-Fuller test) are used to determine if a time series is\n",
    "    stationary or needs differencing.\n",
    "\n",
    "3.  **Financial Applications**: The concept of random walks is\n",
    "    fundamental to the Efficient Market Hypothesis in finance.\n",
    "\n",
    "4.  **Forecasting Challenges**: Random walks are notoriously difficult\n",
    "    to forecast, as the best prediction for any future value is simply\n",
    "    the current value (for a simple random walk).\n",
    "\n",
    "## Example in Python\n",
    "\n",
    "Let’s create an example of both types of random walks and visualize\n",
    "them:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simple_random_walk(steps, start=0):\n",
    "    return start + np.cumsum(np.random.normal(0, 1, steps))\n",
    "\n",
    "def random_walk_with_drift(steps, drift, start=0):\n",
    "    return start + np.cumsum(np.random.normal(drift, 1, steps))\n",
    "\n",
    "# Generate random walks\n",
    "steps = 1000\n",
    "simple_walk = simple_random_walk(steps)\n",
    "drift_walk = random_walk_with_drift(steps, drift=0.1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(simple_walk, label='Simple Random Walk')\n",
    "plt.plot(drift_walk, label='Random Walk with Drift')\n",
    "plt.title('Simple Random Walk vs Random Walk with Drift')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot the differences\n",
    "simple_diff = np.diff(simple_walk)\n",
    "drift_diff = np.diff(drift_walk)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(simple_diff, bins=50, alpha=0.5, label='Simple Random Walk')\n",
    "plt.hist(drift_diff, bins=50, alpha=0.5, label='Random Walk with Drift')\n",
    "plt.title('Distribution of Step Differences')\n",
    "plt.xlabel('Step Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Simple Random Walk - Mean: {np.mean(simple_diff):.4f}, Std Dev: {np.std(simple_diff):.4f}\")\n",
    "print(f\"Random Walk with Drift - Mean: {np.mean(drift_diff):.4f}, Std Dev: {np.std(drift_diff):.4f}\")\n",
    "```\n",
    "\n",
    "This code generates and plots both a simple random walk and a random\n",
    "walk with drift. It also shows the distribution of step differences for\n",
    "both walks.\n",
    "\n",
    "## Interpretation of Results\n",
    "\n",
    "1.  The simple random walk will tend to wander around the starting\n",
    "    point, while the random walk with drift will show a general trend in\n",
    "    the direction of the drift.\n",
    "\n",
    "2.  The distribution of step differences for both walks should be\n",
    "    approximately normal, with the random walk with drift having a mean\n",
    "    close to the drift value.\n",
    "\n",
    "3.  The standard deviation of the step differences should be similar for\n",
    "    both walks.\n",
    "\n",
    "Random walks are fundamental in understanding many real-world phenomena,\n",
    "especially in financial markets and economic indicators. They highlight\n",
    "the challenges in predicting future values based solely on past\n",
    "observations and underscore the importance of considering the underlying\n",
    "process generating the data in time series analysis.\n",
    "\n",
    "### Relationship between Random Walks and Time Series Analysis\n",
    "\n",
    "## 1. Non-Stationarity\n",
    "\n",
    "Random walks are non-stationary processes, which means their statistical\n",
    "properties (like mean and variance) change over time. This is crucial in\n",
    "time series analysis because:\n",
    "\n",
    "-   Many time series models (like ARIMA) assume stationarity.\n",
    "\n",
    "-   Identifying and dealing with non-stationarity is a key step in time\n",
    "    series analysis.\n",
    "\n",
    "-   Techniques like differencing are often used to transform\n",
    "    non-stationary series (like random walks) into stationary ones.\n",
    "\n",
    "## 2. Unit Root Tests\n",
    "\n",
    "Tests for random walks, such as the Augmented Dickey-Fuller (ADF) test,\n",
    "are essential in time series analysis:\n",
    "\n",
    "-   They help determine if a series is stationary or contains a unit\n",
    "    root (indicative of a random walk).\n",
    "\n",
    "-   The presence of a unit root suggests the need for differencing to\n",
    "    achieve stationarity.\n",
    "\n",
    "## 3. Forecasting Challenges\n",
    "\n",
    "Random walks present unique challenges in forecasting:\n",
    "\n",
    "-   For a simple random walk, the best forecast for any future value is\n",
    "    the current value.\n",
    "\n",
    "-   This property underlies the “Efficient Market Hypothesis” in\n",
    "    finance, suggesting that future stock prices can’t be predicted from\n",
    "    past prices.\n",
    "\n",
    "## 4. Model Selection\n",
    "\n",
    "Understanding if a series behaves like a random walk influences model\n",
    "selection:\n",
    "\n",
    "-   If a series is a random walk, models like ARIMA(0,1,0) might be\n",
    "    appropriate.\n",
    "\n",
    "-   For series with drift, ARIMA(0,1,0) with a constant term might be\n",
    "    suitable.\n",
    "\n",
    "## 5. Long-Term Behavior\n",
    "\n",
    "Random walks have specific long-term properties that affect time series\n",
    "analysis:\n",
    "\n",
    "-   The variance of a random walk increases with time, which impacts\n",
    "    long-term forecasting.\n",
    "\n",
    "-   This property is related to the concept of “long memory” in time\n",
    "    series.\n",
    "\n",
    "## 6. Cointegration\n",
    "\n",
    "In multivariate time series analysis, the concept of cointegration is\n",
    "closely related to random walks:\n",
    "\n",
    "-   Two or more non-stationary series that are random walks might have a\n",
    "    stationary linear combination.\n",
    "\n",
    "-   This is crucial in analyzing relationships between economic\n",
    "    variables.\n",
    "\n",
    "Let’s demonstrate some of these concepts with a Python example:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Generate a random walk\n",
    "np.random.seed(42)\n",
    "steps = 1000\n",
    "random_walk = np.cumsum(np.random.normal(0, 1, steps))\n",
    "\n",
    "# Plot the random walk\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(random_walk)\n",
    "plt.title('Random Walk')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform ADF test\n",
    "result = adfuller(random_walk)\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "# Difference the series\n",
    "diff_series = np.diff(random_walk)\n",
    "\n",
    "# Plot the differenced series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(diff_series)\n",
    "plt.title('Differenced Random Walk')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform ADF test on differenced series\n",
    "result = adfuller(diff_series)\n",
    "print('ADF Statistic (differenced):', result[0])\n",
    "print('p-value (differenced):', result[1])\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(random_walk, order=(0,1,0))\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "# Forecast\n",
    "forecast = results.forecast(steps=100)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(random_walk, label='Original Series')\n",
    "plt.plot(range(len(random_walk), len(random_walk) + 100), forecast, label='Forecast')\n",
    "plt.title('Random Walk with Forecast')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This example demonstrates:\n",
    "\n",
    "1.  Generation and visualization of a random walk.\n",
    "\n",
    "2.  Use of the ADF test to check for stationarity.\n",
    "\n",
    "3.  Differencing to transform the non-stationary random walk into a\n",
    "    stationary series.\n",
    "\n",
    "4.  Fitting an ARIMA(0,1,0) model, which is appropriate for a random\n",
    "    walk.\n",
    "\n",
    "5.  Forecasting future values of the random walk.\n",
    "\n",
    "Understanding the relationship between random walks and time series\n",
    "analysis is crucial for proper model selection, interpretation of\n",
    "results, and accurate forecasting in various fields, including\n",
    "economics, finance, and scientific research.\n",
    "\n",
    "**Sources:** - (1) Data-Preprocessing.ipynb - [(2) Introduction to Time\n",
    "Series Metrics -\n",
    "Anodot](https://www.anodot.com/learning-center/time-series-metrics/) -\n",
    "[(3) Understanding Skewness And Kurtosis And How to Plot\n",
    "Them](https://www.datacamp.com/tutorial/understanding-skewness-and-kurtosis) -\n",
    "[(4) PDF Time series outlier detection, a data-driven\n",
    "approach](https://www.bis.org/ifc/publ/ifcb57_07.pdf) - [(5) Akaike\n",
    "information criterion -\n",
    "Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion) -\n",
    "[(6) .6 Random walks (RW) - Applied Time Series\n",
    "Analysis](https://atsa-es.github.io/atsa-labs/sec-tslab-random-walks-rw.html) -\n",
    "[(7) Time Series Analysis: Definition, Types & Techniques -\n",
    "Tableau](https://www.tableau.com/analytics/what-is-time-series-analysis) -\n",
    "[(8) The Complete Guide to Skewness and Kurtosis -\n",
    "Simplilearn.com](https://www.simplilearn.com/tutorials/statistics-tutorial/skewness-and-kurtosis) -\n",
    "[(9) PDF A review on outlier/anomaly detection in time series data -\n",
    "BIRD\n",
    "Home](https://bird.bcamath.org/bitstream/handle/20.500.11824/1305/A_review_on_outlier_detection_in_time_series_data__BCAM_1.pdf.pdf;jsessionid=C2FC2BA5A917804DBEFE4E4A184A52DF?sequence=1) -\n",
    "[(10) Why is AIC or BIC commonly used in model selections for time\n",
    "…](https://stats.stackexchange.com/questions/523485/why-is-aic-or-bic-commonly-used-in-model-selections-for-time-series-forecasting) -\n",
    "[(11) Random Walk Process - CFA, FRM, and Actuarial Exams Study\n",
    "Notes](https://analystprep.com/study-notes/cfa-level-2/random-walk-process/) -\n",
    "[(12) Time Series Data Analysis \\|\n",
    "InfluxData](https://www.influxdata.com/what-is-time-series-data/) -\n",
    "[(13) PDF Tests for Skewness, Kurtosis, and Normality for Time Series\n",
    "Data](http://www.columbia.edu/~jb3064/papers/2005_Testing_skewness_kurtosis_and_normality_for_time_series_data.pdf) -\n",
    "[(14) Algorithm for Online Outlier Detection in Time Series -\n",
    "Baeldung](https://www.baeldung.com/cs/time-series-online-outlier-detection) -\n",
    "[(15) (PDF) Performance Evaluation of AIC and BIC in Time Series\n",
    "…](https://www.researchgate.net/publication/338730979_Performance_Evaluation_of_AIC_and_BIC_in_Time_Series_Clustering_with_Piccolo_Method) -\n",
    "[(16) White Noise and Random Walks in Time Series Analysis -\n",
    "QuantStart](https://www.quantstart.com/articles/White-Noise-and-Random-Walks-in-Time-Series-Analysis/) -\n",
    "[(17) Time-Series Analysis: What Is It and How to Use It -\n",
    "Timescale](https://www.timescale.com/blog/time-series-analysis-what-is-it-how-to-use-it) -\n",
    "[(18) Understanding Basic Statistical Values (Advanced Signal\n",
    "…](https://www.ni.com/docs/en-US/bundle/labview-advanced-signal-processing-toolkit-api-ref/page/lvasptconcepts/tsa_basic_stat_analysis.html) -\n",
    "[(19) Chapter 5 Outlier detection in Time\n",
    "series](https://s-ai-f.github.io/Time-Series/outlier-detection-in-time-series.html) -\n",
    "(20) Time Series Model Selection (AIC & BIC) - YouTube - [(21) A Gentle\n",
    "Introduction to the Random Walk for Times Series\n",
    "…](https://www.machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/)\n",
    "\n",
    "### Random Walks and Short-term Forecasting\n",
    "\n",
    "Random walks can be used to improve short-term forecasting in time\n",
    "series data in several ways. While random walks themselves are\n",
    "unpredictable by nature, understanding and incorporating their\n",
    "properties can enhance forecasting models, especially for short-term\n",
    "predictions. Here’s how:\n",
    "\n",
    "1.  Baseline Model:  \n",
    "    Random walks serve as an important baseline model for short-term\n",
    "    forecasting. The naive forecast (using the last observed value as\n",
    "    the prediction for all future values) is often based on the random\n",
    "    walk model. This baseline helps in evaluating the performance of\n",
    "    more complex models.\n",
    "\n",
    "2.  Differencing:  \n",
    "    Many time series exhibit random walk-like behavior. By differencing\n",
    "    the data (subtracting each observation from the next), we can\n",
    "    transform a non-stationary random walk into a stationary series.\n",
    "    This stationary series is often easier to forecast.\n",
    "\n",
    "3.  ARIMA Modeling:  \n",
    "    Random walks are effectively modeled by ARIMA(0,1,0) processes. This\n",
    "    knowledge can guide the selection of appropriate ARIMA parameters\n",
    "    for short-term forecasting.\n",
    "\n",
    "4.  Drift Component:  \n",
    "    Incorporating a drift term (as in a random walk with drift) can\n",
    "    capture small, consistent trends in the data, improving short-term\n",
    "    forecasts.\n",
    "\n",
    "5.  Ensemble Methods:  \n",
    "    Combining random walk forecasts with other models in an ensemble can\n",
    "    improve overall forecast accuracy, especially in the short term.\n",
    "\n",
    "Let’s demonstrate some of these concepts with a Python example:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Generate a random walk with drift\n",
    "np.random.seed(42)\n",
    "steps = 1000\n",
    "drift = 0.1\n",
    "random_walk = np.cumsum(np.random.normal(drift, 1, steps))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(random_walk) * 0.8)\n",
    "train, test = random_walk[:train_size], random_walk[train_size:]\n",
    "\n",
    "# Naive forecast (random walk model)\n",
    "naive_forecast = np.full(len(test), train[-1])\n",
    "\n",
    "# ARIMA(0,1,0) with drift\n",
    "model_arima = ARIMA(train, order=(0,1,0), trend='c')\n",
    "results_arima = model_arima.fit()\n",
    "forecast_arima = results_arima.forecast(steps=len(test))\n",
    "\n",
    "# Simple exponential smoothing\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "model_ses = SimpleExpSmoothing(train).fit()\n",
    "forecast_ses = model_ses.forecast(len(test))\n",
    "\n",
    "# Ensemble forecast (average of ARIMA and SES)\n",
    "forecast_ensemble = (forecast_arima + forecast_ses) / 2\n",
    "\n",
    "# Calculate RMSE for each method\n",
    "rmse_naive = sqrt(mean_squared_error(test, naive_forecast))\n",
    "rmse_arima = sqrt(mean_squared_error(test, forecast_arima))\n",
    "rmse_ses = sqrt(mean_squared_error(test, forecast_ses))\n",
    "rmse_ensemble = sqrt(mean_squared_error(test, forecast_ensemble))\n",
    "\n",
    "print(f'RMSE Naive: {rmse_naive:.4f}')\n",
    "print(f'RMSE ARIMA: {rmse_arima:.4f}')\n",
    "print(f'RMSE SES: {rmse_ses:.4f}')\n",
    "print(f'RMSE Ensemble: {rmse_ensemble:.4f}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(train)), train, label='Train')\n",
    "plt.plot(range(len(train), len(random_walk)), test, label='Test')\n",
    "plt.plot(range(len(train), len(random_walk)), naive_forecast, label='Naive Forecast')\n",
    "plt.plot(range(len(train), len(random_walk)), forecast_arima, label='ARIMA Forecast')\n",
    "plt.plot(range(len(train), len(random_walk)), forecast_ses, label='SES Forecast')\n",
    "plt.plot(range(len(train), len(random_walk)), forecast_ensemble, label='Ensemble Forecast')\n",
    "plt.title('Random Walk with Drift: Various Forecasting Methods')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This example demonstrates:\n",
    "\n",
    "1.  Using a naive forecast based on the random walk model as a baseline.\n",
    "\n",
    "2.  Applying an ARIMA(0,1,0) model with drift, which is suitable for\n",
    "    random walks.\n",
    "\n",
    "3.  Using Simple Exponential Smoothing as an alternative method.\n",
    "\n",
    "4.  Creating an ensemble forecast by averaging the ARIMA and SES\n",
    "    forecasts.\n",
    "\n",
    "The results typically show that:\n",
    "\n",
    "-   The ARIMA model with drift often performs well for short-term\n",
    "    forecasts of random walk-like data.\n",
    "\n",
    "-   The ensemble method can provide improved forecasts by combining\n",
    "    different approaches.\n",
    "\n",
    "-   Even simple methods like SES can be effective for short-term\n",
    "    forecasting of random walks.\n",
    "\n",
    "By understanding and leveraging the properties of random walks, we can\n",
    "develop more effective short-term forecasting strategies for time series\n",
    "data that exhibit random walk-like behavior.\n",
    "\n",
    "**Sources:** - (1) Data-Preprocessing.ipynb - [(2) Introduction to Time\n",
    "Series Metrics -\n",
    "Anodot](https://www.anodot.com/learning-center/time-series-metrics/) -\n",
    "[(3) Understanding Skewness And Kurtosis And How to Plot\n",
    "Them](https://www.datacamp.com/tutorial/understanding-skewness-and-kurtosis) -\n",
    "[(4) PDF Time series outlier detection, a data-driven\n",
    "approach](https://www.bis.org/ifc/publ/ifcb57_07.pdf) - [(5) Akaike\n",
    "information criterion -\n",
    "Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion) -\n",
    "[(6) .6 Random walks (RW) - Applied Time Series\n",
    "Analysis](https://atsa-es.github.io/atsa-labs/sec-tslab-random-walks-rw.html) -\n",
    "[(7) Time Series Analysis: Definition, Types & Techniques -\n",
    "Tableau](https://www.tableau.com/analytics/what-is-time-series-analysis) -\n",
    "[(8) The Complete Guide to Skewness and Kurtosis -\n",
    "Simplilearn.com](https://www.simplilearn.com/tutorials/statistics-tutorial/skewness-and-kurtosis) -\n",
    "[(9) PDF A review on outlier/anomaly detection in time series data -\n",
    "BIRD\n",
    "Home](https://bird.bcamath.org/bitstream/handle/20.500.11824/1305/A_review_on_outlier_detection_in_time_series_data__BCAM_1.pdf.pdf;jsessionid=C2FC2BA5A917804DBEFE4E4A184A52DF?sequence=1) -\n",
    "[(10) Why is AIC or BIC commonly used in model selections for time\n",
    "…](https://stats.stackexchange.com/questions/523485/why-is-aic-or-bic-commonly-used-in-model-selections-for-time-series-forecasting) -\n",
    "[(11) Random Walk Process - CFA, FRM, and Actuarial Exams Study\n",
    "Notes](https://analystprep.com/study-notes/cfa-level-2/random-walk-process/) -\n",
    "[(12) Time Series Data Analysis \\|\n",
    "InfluxData](https://www.influxdata.com/what-is-time-series-data/) -\n",
    "[(13) PDF Tests for Skewness, Kurtosis, and Normality for Time Series\n",
    "Data](http://www.columbia.edu/~jb3064/papers/2005_Testing_skewness_kurtosis_and_normality_for_time_series_data.pdf) -\n",
    "[(14) Algorithm for Online Outlier Detection in Time Series -\n",
    "Baeldung](https://www.baeldung.com/cs/time-series-online-outlier-detection) -\n",
    "[(15) (PDF) Performance Evaluation of AIC and BIC in Time Series\n",
    "…](https://www.researchgate.net/publication/338730979_Performance_Evaluation_of_AIC_and_BIC_in_Time_Series_Clustering_with_Piccolo_Method) -\n",
    "[(16) White Noise and Random Walks in Time Series Analysis -\n",
    "QuantStart](https://www.quantstart.com/articles/White-Noise-and-Random-Walks-in-Time-Series-Analysis/) -\n",
    "[(17) Time-Series Analysis: What Is It and How to Use It -\n",
    "Timescale](https://www.timescale.com/blog/time-series-analysis-what-is-it-how-to-use-it) -\n",
    "[(18) Understanding Basic Statistical Values (Advanced Signal\n",
    "…](https://www.ni.com/docs/en-US/bundle/labview-advanced-signal-processing-toolkit-api-ref/page/lvasptconcepts/tsa_basic_stat_analysis.html) -\n",
    "[(19) Chapter 5 Outlier detection in Time\n",
    "series](https://s-ai-f.github.io/Time-Series/outlier-detection-in-time-series.html) -\n",
    "(20) Time Series Model Selection (AIC & BIC) - YouTube - [(21) A Gentle\n",
    "Introduction to the Random Walk for Times Series\n",
    "…](https://www.machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/)"
   ],
   "id": "2087ba16-16da-4bbf-9099-adc91a4e74fc"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
